from fastapi import APIRouter, HTTPException
from typing import Dict, Any, List
from pydantic import BaseModel
from dotenv import load_dotenv
import os
from src.services.retrieving import retrieve
from src.services.llm_service import llm_service
import logging

# Load environment variables
load_dotenv()

logger = logging.getLogger(__name__)

router = APIRouter()

class ChatMessageRequest(BaseModel):
    content: str
    context_window: int = 5  # Number of context chunks to retrieve

class ChatSessionRequest(BaseModel):
    title: str = "New Chat Session"

@router.post("/sessions")
async def create_chat_session(request: ChatSessionRequest = None):
    """Create a new chat session"""
    if request is None:
        request = ChatSessionRequest()

    # In a real implementation, you would create a session in the database
    # For now, we'll return a sample session
    session_id = "session_" + str(hash(request.title))[:8]
    return {"session_id": session_id, "title": request.title, "created_at": "2025-12-11T00:00:00Z"}

@router.post("/sessions/{session_id}/messages")
async def send_chat_message(session_id: str, request: ChatMessageRequest):
    """Send a message in a chat session and get RAG-enhanced response"""
    try:
        # Use the retrieving service to get relevant content
        retrieved_texts = retrieve(request.content, limit=request.context_window)

        # Format the context from retrieved chunks
        context_text = ""
        sources = []
        for text in retrieved_texts:
            context_text += f"\n\n{text}"
            sources.append({
                "content": text,
                "title": "Retrieved Content",
                "file_path": "unknown",
                "score": 1.0  # Placeholder score
            })

        # Create the system message with context
        system_message = f"""You are an AI assistant for the Physical AI & Humanoid Robotics Textbook.
        Use the following context to answer the user's question.
        If the context doesn't contain relevant information, say so.
        Be accurate, concise, and cite sources when possible.

        Context: {context_text}"""

        # Generate response using configured LLM provider
        messages = [
            {"role": "system", "content": system_message},
            {"role": "user", "content": request.content}
        ]

        ai_response = llm_service.chat_completion(
            messages=messages,
            temperature=0.3,
            max_tokens=1000
        )

        return {
            "session_id": session_id,
            "response": ai_response,
            "sources": sources,
            "query": request.content,
            "retrieved_chunks": len(relevant_chunks)
        }

    except Exception as e:
        logger.error(f"Error processing chat message: {e}")
        raise HTTPException(status_code=500, detail=f"Error processing chat message: {str(e)}")

@router.post("/ask-from-selection")
async def ask_from_selection(request: ChatMessageRequest):
    """Ask a question about selected/highlighted text"""
    try:
        # Use the retrieving service to get relevant content
        retrieved_texts = retrieve(request.content, limit=request.context_window)

        # Format the context from retrieved chunks
        context_text = ""
        sources = []
        for text in retrieved_texts:
            context_text += f"\n\n{text}"
            sources.append({
                "content": text,
                "title": "Retrieved Content",
                "file_path": "unknown",
                "score": 1.0  # Placeholder score
            })

        # Create the system message with context
        system_message = f"""You are an AI assistant for the Physical AI & Humanoid Robotics Textbook.
        A user has highlighted/selected the following text and asked about it: "{request.content}"
        Use the following context to provide a detailed explanation or answer.
        If the context doesn't contain relevant information, say so.
        Be accurate, concise, and cite sources when possible.

        Context: {context_text}"""

        # Generate response using configured LLM provider
        messages = [
            {"role": "system", "content": system_message},
            {"role": "user", "content": f"Please explain or answer about: {request.content}"}
        ]

        ai_response = llm_service.chat_completion(
            messages=messages,
            temperature=0.3,
            max_tokens=1000
        )

        return {
            "response": ai_response,
            "sources": sources,
            "selected_text": request.content,
            "retrieved_chunks": len(relevant_chunks)
        }

    except Exception as e:
        logger.error(f"Error processing selection-based question: {e}")
        raise HTTPException(status_code=500, detail=f"Error processing selection-based question: {str(e)}")